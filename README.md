# StereoRL
Unsupervised generation of stereo disparity maps using reinforcement learning and no ground truth

**Depends on**
- opencv
- numpy
- matplotlib
- tensorflow
- keras
- numba for cuda.  This is cool package that allows you to write CUDA kernels directly in your python scripts.

**Setup**
- Create state-saves folder
- If you want to training the classifier, create a folder called "classifier_checkpoints"
- If you want to test a pretrained model. unzip it first
- for now follow instructions inside the training and testing folders for copying and converting data

Scripts that begine with "rl_x_reduced" are the reinforcement learning training agent model training scripts.

## Introduction
The purpose of this effort was to develop a method that can train a model to produce stereo disparity maps without ground truth disparity maps.  The unsupervised metric used was image reconstruction.  Additionally, reinforcement learning was used and the problem formulated as a non-linear optimization problem.  Our goal is to evolve this method to non-aligned image pairs and optical flow.  However, stereo pairs are a simpler case and we used KITTI 2015 for this project.  This project report compares results to a state-of-the-art method that relies on labelled data, Hierarchical Neural Architecture Search for Deep Stereo Matching.  We discuss the primary problem in reinforcement learning, predicting future reward, and limitation of our unsupervised reconstruction metric.  We outline which steps and potential improvements we will experiment with next.  Code for this project can be found here:  https://github.com/feildawproton/StereoRL 

## Background
The production of disparity maps and optical flow maps have historically been done with optimization methods and manual feature descriptors.  Deep learning has dominated vision classification tasks for about 8 years now.  Since 2016 it has dominated stereo disparity and optical flow tasks.  However, most methods require ground truth for training which reduces the availability of data.  More recent research has been done into unsupervised methods.  We explore an unsupervised method in the form of reinforcement learning. 
Hsueh-Ying Lai et al. in their 2019 paper “Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence” [1] used and unsupervised metric for producing stereo disparity and optical flow maps.  They train a model to jointly learn stereo disparity and optical flow.  They introduce geometric constraints on the learning process and utilized in their objective for unsupervised training.  They use a warping function applied to the images and their 2-Warp loss measure consistency between disparity and optical flow tasks.  The unsupervised loss functions they use include: self-supervised reconstruction loss, smoothness loss, left-right consistency loss, and 2-Warp consistency loss.  The reconstruction loss they use is occlusion aware.  The smoothness loss encourages local smoothness but allows edges.  This method is state-of-the-art and we wanted to get it working but were not able to do so in time
Ryosuke Furuta and Toshihiko Yamasaki introduced PixelRL [6] to address the problem of action space size when applying reinforcement learning to image tasks.  The use a fully convolutional neural network (FCN) that copies parameter across all pixels, including the action space for each pixel.  This greatly reduced the memory requirement of the model (compared to one that had action spaces that addressed each pixel separately (instead of sharing weights)).  PixelRL formalized each pixel as having and agent.  We were greatly inspired by PixelRL to reduce the memory cost when using reinforcement learning for image operations.  We don’t formalize each pixel as having an agent, but in-the-end the effect is quite similar.
Junyuan Xie et al. in their 2016 paper “Deep3D:  Fully Automatic 2d-to-3D Video Conversion with Deep Convolutional Neural Networks” [2] introduce a method to produce stereo image pairs from single 2D images.  What is noteworthy about their approach is that it is trained in a unsupervised manner on 3D movies.  This means that there is a huge number of training samples.  This paper is of note to us because they also use an unsupervised image reconstruction metric in order to train the production of disparity maps.
Wenjie Luo et al. in their 2016 paper “Efficient Deep Learning for Stereo Matching” [8] demonstrate what is now considered an archetypal supervised disparity map deep learning method.  This method using Siamese networks on image pairs and ground truth labels.  We investigated this method because it is considered now typical and the code is openly available.  We tried getting their code working but this would have required extensive porting.
Moritz Menze and Andreas Geiger introduce the KITTI 2015 in their 2015 paper “Object Scene Flow for Autonomous Vehicles” [4].  They use LIDAR and fitting 3D models to objects in scenes in order to produce ground truth disparity and optical flow maps.  This is the dataset that we use because many methods rely on ground truth data, even though our method does not.  The benchmark is available here: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo .  Test disparity and optical flow maps can be submitted to the benchmark for evaluation.  Alternative dataset exist such as those introduce by Nikolaus Mayer et al [5].  These datasets use computer generated graphics to produce datasets with absolutely accurate ground truth.  These dataset can be found here: https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html 
Xuelian Cheng et al. introduced LEAStereo in their 2020 paper “Hierarchical Neural Architecture Search for Deep Stereo Matching” [9].  This method implements a Neural Architecture Search (NAS) that searches a set of network operations in order to find the best architecture for a given task.  In this case it has been applied to stereo matching, including for the KITTI 2015 dataset.  This method requires ground truth.   We were able to get the code for this method working.  Additionally, the team provide their best results which we used for comparison to our method.  This method is the most performant method on KITTI that also has its code available for download.  
Saad Merrouche et al. describe the most common methods used to evaluate disparity maps in their 2020 paper “Objective Image Quality Measures for Disparity Maps Evaluation” [7].  Even though we did not address the quality of our disparity maps directly in this project, our disparity maps will be assessed when we submit them to benchmarks that rely on labeled, ground truth data.
Kun Zhou et al. review the state-of-the-art in stereo matching algorithms in their 2020 paper “Review of Stereo Matching Algorithms Based on Deep Learning” [3].  The go over traditional optimization and search methods but focus on how deep learning is used today.  The identify three categories: 1) using deep learning to augment a traditional algorithm, such as substituting CNNs in for feature learning only; 2) Deep learning methods relying on labeled data, and; 3) Unsupervised methods.  They note that all unsupervised methods rely on minimizing error between warped versions of the input image.  This can be reformulated to describe our approach of image reconstruction.

# Methods
## Overview
The primary ideas behind this project are: 1) to use an unsupervised reward/error function for disparity/flow map production, in order to enable learning and testing on unlabeled data; 2) to allow for online learning/lifelong learning and better fitment to testing or real world data because of the unsupervised reward function; 3) to use reinforcement learning, so that the agent model can make iterative improvements to its disparity map using the reward function; 4) to minimize the size of the action space using a fully convolution neural network.
Based on our goals we cannot rely on ground truth disparity maps; the unsupervised reward function used was image reconstruction.  The model produces iterative changes to a disparity map.  Disparity maps index the pixels on one image to the pixels on another image.  In our case the disparity map is aligned with the left image and points to the right image.  Therefore we can hypothesize that reconstruction of the left image, using the disparity map and right image, is an unsupervised stand-in for the disparity map.
Because we are using an unsupervised reward function we can enable online learning.  This allows the model iterate and improve over test/deployed datasets.  As long as the model does not over-fit and lose generality an agent model can perform better over time with more iterations over user data.
Reinforcement learning is a natural fit for iterative improvements and unsupervised reward functions.   While to agent model in reinforcement learning is optimized by its reward function, the agent itself is an optimizer.  The agent optimizes the environment, through its action space, and makes adjustments based on the environment and its internal representation of the reward function.  This makes reinforcement learning useful for optimization-like problems.  
In our case we formulated to production of disparity maps as an optimization problem.  RL agents cannot produced something multivalued like a disparity map.  Instead they produce probabilities for taking certain actions.  In our case these actions were changes to the disparity map.  The disparity map starts as all zero indices (pointing to their own locations) and the agent outputs probabilities for changes to every pixel.  These probabilities are maxed and each pixel is either incremented (index increased by one pixel location, decrement (index decreased by one pixel location), or nothing occurs.   We don’t report experimental results for allowing the model to do nothing vs not.  However, anecdotal experience suggests that allowing the model to do nothing is very important to stable learning. 
One of the problems with reinforcement learning is the size of the action space.  As the action space grows, memory demands increase enormously.  We potentially have an unwieldly action space with potential actions for each pixel.  We adopt the fully convolution neural network, inspired by PixelRL [6], in order to enable complete weight sharing.  In this way weights are shared between every pixel, and every pixel effective has an agent.  Because of this weight sharing, we can have much larger inputs and outputs than would otherwise be possible.  The size of the model still has an effect on GPU memory when it is deployed for training due to copying into cache.  Also, image size still has an effect on memory consumption, but its exponent is much reduced.  For example: before moving to a FCN model a single copy of a model taking in 84x84 pixel images would take 8GB of GPU memory on our computer.  Now, with a FCN, we can hold batches of 64 84x278 images in the same amount of GPU memory.



